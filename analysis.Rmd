---
title: "Top 50 Analysis"
date: "`r paste0('Last Updated: ', format(Sys.time(), '%d %B, %Y')) `"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
editor_options: 
  chunk_output_type: console
---

```{r setup, include = F}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
knitr::opts_chunk$set(fig.align = 'center')

library(tidyverse)
library(patchwork)
# library(pdwtheme)
library(gt)
library(infer)
# To Turn off Scientific Notation Use this.
options(scipen = 999)

source("./src/spacee_theme.R")

theme_set(theme_bw())
```


# Executive Summary {.tabset .tabset-pills}

<div class = "summaryblock">

## Prompt

__Prompt:__

Perform the following with the attached “top50.csv” file:   

* Load the dataset and perform EDA and determine if any cleanup (data conversions, name changes, scaling, imputation of missing values, etc.) is needed.  
* Determine if any correlation exists between features.  
* Visualize and examine the distribution of popularity scores and show if any outliers or skewing exists.  
* Perform any transformations as needed and create a model to predict the popularity of a song, while avoiding overfitting and showing relevant scores.

## Goal  

__Goal:__  

We want to learn more about the impact of different measures on a song's popularity, and ideally be able to then use that knowledge to be able to predict what would make a highly popular song

 

 
## Results

__Results:__  
From the analysis we find that:

1. Overall,  


__Potential Investigation Next Steps:__ 

* It would be interesting in future iterations, 

## Assumptions 

__Assumptions:__
_(Note: this section would likely take a different shape as familiarity with the data source increase/ relationship with the customer develops)_ 

With this analysis we are assuming that: 



</div>



# Overview of Data {.tabset .tabset-pills}
Load the dataset and perform EDA and determine if any cleanup (data conversions, name changes, scaling, imputation of missing values, etc.) is needed. 

## Load Data
 
```{r}
top50 <- read_csv(here::here("inputs", "top50.csv")) %>% 
  select(-...1) %>% 
  mutate(CleanTrack = iconv(Track.Name, to = "utf-8"))

names(top50) <- gsub("\\.", "", names(top50)) # ro remove periods from column names

```


## Review ggpairs for variables
Looks like upon initial review that there are decent correlations between:  

* BPM and Speechiness  
* Energy and Loudness  
* Energy and Valence  
* Energy and Acousticness  
* Popularity and Valence  

I am not sure what "Valence" means, so I would probably want to look this up to understand it a little more as it seems to be coorelated to Popularity (which is what we are trying to model)

Each significance level is associated to a symbol : p-values(0, 0.001, 0.01, 0.05, 0.1, 1) <=> symbols(“***”, “**”, “*”, “.”, " “)

```{r, fig.width=10, fig.height=10, out.width="100%"}
GGally::ggpairs(top50, columns = 4:13)
```

## Review Genre

Looked to see if there are general groups the different genres fall within.   

* There are many pop songs as well as multiple pop categories. It's possible that the category of a song could impact it's popularity, but that analysis is likely one for a larger dataset.


```{r}

top50 %>% 
  mutate(GenreGroup = case_when(str_detect(Genre, "pop") ~ "pop",
                                str_detect(Genre, "hip hop") ~ "hip hop",
                                str_detect(Genre, "rap") ~ "rap",
                                str_detect(Genre, "reggaeton") ~ "reggaeton",
                                TRUE ~ "Other")) %>% 
  count(GenreGroup,Genre, sort=T) %>% 
  arrange(GenreGroup, n) %>% 
  DT::datatable()

```



# Coorelation  Review
Determine if any correlation exists between features.  


```{r}
corTest <- top50[,4:13]
res <- cor(corTest)  

col<- colorRampPalette(c("blue", "white", "red"))(20)
heatmap(x = res, col = col, symm = TRUE)

```

```{r}
flattenCorrMatrix <- function(cormat, pmat) {
  ut <- upper.tri(cormat)
  data.frame(
    row = rownames(cormat)[row(cormat)[ut]],
    column = rownames(cormat)[col(cormat)[ut]],
    cor  =(cormat)[ut],
    p = pmat[ut]
    )
}
res2<-Hmisc::rcorr(as.matrix(top50[,4:13]))

corLong <- flattenCorrMatrix(res2$r, res2$P) %>% 
  arrange(p) 

```

Table with the correlations with a p-value < 0.05

```{r}
corLong %>% 
  filter(p < 0.05)
```



# Popularity Scores
Visualize and examine the distribution of popularity scores and show if any outliers or skewing exists.  

Decided to overlay a violin and a box plot.   

* It looks like the popularity scores range from 70 to 95, with the majority of the scores falling more within the 85 to 92 range.  
* Accordingly the two lowest scores of 70 and 78 fall out of the lower quartile, and could be considered outliers in this dataset. 


```{r}
top50 %>% 
  ggplot(aes(x= "", y= Popularity))+
  geom_violin(alpha = 0.2, fill = "grey")+
  geom_boxplot(alpha = 0.6)+
  labs(title = "Distribution of Popularity")
```


# Predict Popularity {.tabset .tabset-pills}
Perform any transformations as needed and create a model to predict the popularity of a song, while avoiding overfitting and showing relevant scores.


## Build training and test splits
```{r}
library(tidymodels)

song_split <- initial_split(top50[,4:13], prop = 0.8)

song_train <- training(song_split)
song_test  <- testing(song_split)

song_val <- validation_split(song_train, prop = 4/5)
song_val$splits[[1]]
```

```{r, eval=F}
tmwr_cols <- colorRampPalette(c("#91CBD765", "#CA225E"))

song_train %>% 
  select(-Popularity) %>%
  cor() %>% 
  corrplot::corrplot(col = tmwr_cols(200), tl.col = "black", method = "ellipse")
```


## Adjust for coorelations
Knowing that some of the variables are correlated, will then 

```{r}
library(bestNormalize)

song_rec <-
  # Use the training data from the bean_val split object
  recipe(Popularity ~ ., data = analysis(song_val$splits[[1]])) %>%
  step_zv(all_numeric_predictors()) %>%
  step_orderNorm(all_numeric_predictors()) %>% 
  step_normalize(all_numeric_predictors())

song_rec_trained <- prep(song_rec)
song_rec_trained
```

```{r}

library(ggforce)
plot_validation_results <- function(recipe, dat = assessment(song_val$splits[[1]])) {
  recipe %>%
    # Estimate any additional steps
    prep() %>%
    # Process the data (the validation set by default)
    bake(new_data = dat) %>%
    # Create the scatterplot matrix
    ggplot(aes(x = .panel_x, y = .panel_y)) + # ,color = Popularity, fill = Popularity
    geom_point(alpha = 0.4, size = 0.5) +
    geom_autodensity(alpha = .3) +
    facet_matrix(vars(-Popularity), layer.diag = 2) + 
    scale_color_brewer(palette = "Dark2") + 
    scale_fill_brewer(palette = "Dark2")
}


song_rec_trained %>%
  step_pca(all_numeric_predictors(), num_comp = 4) %>%
  plot_validation_results() + 
  ggtitle("Principal Component Analysis")

```

```{r}
library(learntidymodels)
song_rec_trained %>%
  step_pca(all_numeric_predictors(), num_comp = 4) %>% 
  prep() %>% 
  plot_top_loadings(component_number <= 4, n = 5) + #uses the learntidymodels
  scale_fill_brewer(palette = "Paired") +
  ggtitle("Principal Component Analysis")
```


## Build Model

```{r}

```









